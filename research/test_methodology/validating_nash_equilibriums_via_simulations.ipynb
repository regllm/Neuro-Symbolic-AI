{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating Nash Equilibriums Via Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_by Colin Manko_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an effort to validate and test possible improvements to core poker artificial intelligence algorithms, I have designed the following methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Validate that MCCFR offline learning strategy is approximating a Nash equilibrium\n",
    "- More generally, create a methodology that allows for rigorously testing changes made to the core AI algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Need two test bot implementation strategies ($\\beta{1}$ and $\\beta{2}$) that we would like to compare\n",
    "- Need a human tester ($H_{0}$) as a quasi control. The human tester should not have access to any underlying strategies from the test bots or simulated Nash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1:  Randomly Generate Test Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of $N$ game tree nodes (this is the entire game tree, as given by infoset, $I$), randomly generate $x$ test nodes without preplacement and with equal probability. Call the set of test nodes $U$.\n",
    "\n",
    "As a side note, we will account for probability of reach ($p(h)$) in another step. Equal probability across nodes allows us to find patterns across nodes where our agent underperforms. We will adjust the expected value at $I$, ($v^{\\sigma}(I)$), by $p(h)$.\n",
    "- **How to**: For Limit Texas Hold'em, the number of action sequences ($N$), is small enough that they can be found computationally rather than analytically. We can run *all_action_sequences.py* in the *size_of_problem* directory to generate this list. \n",
    "- _Something like 15-20 hours and less than 4GB??_\n",
    "- Generate $x$ integers to be indices and select them from the *all_action_sequences.py* output\n",
    "- Once $x$ action sequences are generated, randomly generate $x$ public card combos, based on the betting stage of the test node, $u$, as well as one pair of private hole cards to be used by $\\beta{1}$, $\\beta{2}$ and $H_0$. They will only get that hand at $u$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Prepare Realtime Search for Finding Nash Equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each test node, $u$, in $U$, use realtime search to compute the Nash Equilibrium ($\\sigma^*$) by constraining the search algorithm to start at $u$, where $u$ is equivalent to $I$ in regard to action sequence, but does not have any set hand for the traversing player ($p_i$).\n",
    "\n",
    "Use a pooled strategy between $\\beta{1}$ and $\\beta{2}$ to estimate $p(h)$ without bias:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hand in possible combinations of real hands:\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For idx, $a$ in action sequence at $u$:\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if idx == 0:\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$p(h)_\\beta{1}$ = $\\beta{1}$[$I$][$a$]\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$p(h)_\\beta{2}$ = $\\beta{2}$[$I$][$a$]\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$p(h)_\\beta{1}$ *= $\\beta{1}$[$I$][$a$]\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$p(h)_\\beta{2}$ *= $\\beta{1}$[$I$][$a$]\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;p(h)[rs] = ($p(h)_\\beta{1}$ + $p(h)_\\beta{2}$)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The root node of the realtime search algorithm is replaced with a chance node that represents each possible node in the public state $G$ [[Brown, Sandholm, Amos]](https://papers.nips.cc/paper/7993-depth-limited-solving-for-imperfect-information-games.pdf). From the above psuedo-code, this deal can be generated as: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate deal order\n",
    "<br>\n",
    "For $i$ in $P_i$ deal order:\n",
    "<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Generate hand for player based on normalized $p(h)[rs]$ if available, else try again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The \"if available, else try again\" part could be made better_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other important features of the _\"Nash Bot\"_ real time search..**:\n",
    "- The _\"Nash bot\"_ is the master of this node. In order to reach full convergence, from the normal MCCFR algorithm, we must remove the sampling of actions for opponents.\n",
    "- For ease, the real time search should not use leaf nodes, but should search to the end of the game tree, where either a terminal node or a shown down is entered. In this way, we can get a truer sense of the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Nash Equilibrium is found if the change in strategy on each iteration drops below some threshold $t$ for the real hand we are testing for. Charting probabilities for each action in $u$ over time for the randomly generated real hand to test should show a convergence over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_One main benefit of using this real time search to validate CFR is this search will need to be developed anyway._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test and Measure Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the test bots:**\n",
    "For each $u$ in $U$, play each test strategy ($\\beta{1}$ and $\\beta{2}$) against the _\"Nash bot\"_ for $r$ number of simulations. The _\"Nash bot\"_ should be dealt available hands from the distribution of probabilities as determined by $p(h)[rs]$ in the pseudo-code above. Both the test bots and the human tester will be dealt the same hand in each simulation of game play on $u$, as randomly generated in step 1. \n",
    "\n",
    "If $\\beta{1}$ or $\\beta{2}$ has converged to a Nash equilibrium, then we should expect $v^\\sigma$ to be equal to 0 for our test bot, assuming that _\"Nash bot\"_ has converged to a Nash equilibrium itself. $v^{\\sigma^*}(u)$ and $v^{\\sigma}(u)$ are the estimated payouts for the _\"Nash bot\"_ opponents and the \"hero\" (test bots or human), respectively.\n",
    "\n",
    "**For the human tester:**\n",
    "We can simply create a contrived game. Based on the normalized probability of reach for $u$, $\\bar{p(h)}$, we can randomly generate which $u$ the human player is entered into, however they will always have the same hand upon entering $u$ and their opponents hands will vary based on $p(h)[rs]$.\n",
    "\n",
    "The test metric is as follows, after $p(h)$ has been normalized for space $U$, $\\bar{p(h)}$:\n",
    "$$\\sum_{i=1}^{x}(v^{\\sigma}(u_i)-v^{\\sigma^*}(u_{-i}))\\times{\\bar{p(h)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value closest to 0 (summing no testing agent goes over 0) will have best approximated the Nash equilibrium. Additionaly, $H_0$ can be used as a quasi-control, to validate that the bot is beating a human.\n",
    "\n",
    "The above metric also has some degree of simulation error. For each simulation in $r$ simulations, we create a distribution of values that has a standard deviation and follows the normal distribution. \n",
    "\n",
    "Along with calculating the expected payout per simulation, $u^{\\sigma}(u_i)-u^{\\sigma^*}(u_{-i})$, we can also calculate $\\sigma$ for this distribution in order to describe a confidence interval around the test metric. \n",
    "\n",
    "Finally, a simple difference of means can be done between each test bot to decipher a winner and if that winner had a statistically significant edge. We can then study each $u$ in $U$ to find patterns in which nodes the espspective bots did not do well with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
